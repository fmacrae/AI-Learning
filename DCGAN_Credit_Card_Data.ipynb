{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN Credit Card Data.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/fmacrae/AI-Learning/blob/master/DCGAN_Credit_Card_Data.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "lzkpnqcI1YcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###SETUP\n",
        "Connect to Google Drive to store the model and access data.\n",
        "\n",
        "1.   If setting this to false watch out for discconections to the VM - It's epherial though so you'll lose all your checkpoints\n",
        "2.   Also define how much training to do\n",
        "\n",
        "Based on the work by Arthur Juliani, check out his other work! https://github.com/awjuliani/TF-Tutorials \n",
        "\n",
        "Changes:\n",
        "\n",
        "1.   Integrated with Google Drive to allow storage of model checkpoints and sample outputs\n",
        "2.   Moved onto Collabatory - Make sure you select runtime with GPU\n",
        "3.   Improved resiliancy by allowing it to restart from last checkpoint\n",
        "4.   Moved setup and training options to top for easier setup.\n",
        "5.   Reworking to use credit card data from https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
        "\n",
        "Useful Links:  \n",
        "\n",
        "https://github.com/soumith/ganhacks\n",
        "\n",
        "https://sigmoidal.io/beginners-review-of-gan-architectures/\n",
        "\n",
        "https://www.toptal.com/machine-learning/generative-adversarial-networks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WcbWsqIlsqgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_cloud_storage = True ## change to false if you want to just store the images and checkpoints on the colab VM. Will need to get the cc data on manually though \n",
        "iterations = 1001 #Total number of iterations to use.  If you go lower than 1001 then no checkpoint is stored.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1pn0bGw1WWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2366
        },
        "outputId": "04dc2c69-0ac1-4302-99a7-7f1b8280f2d7"
      },
      "cell_type": "code",
      "source": [
        "if use_cloud_storage:\n",
        "   !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "   !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "   !apt-get update -qq 2>&1 > /dev/null\n",
        "   !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "   from google.colab import auth\n",
        "   auth.authenticate_user()\n",
        "   from oauth2client.client import GoogleCredentials\n",
        "   creds = GoogleCredentials.get_application_default()\n",
        "   import getpass\n",
        "   !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "   vcode = getpass.getpass()\n",
        "   !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package cron.\n",
            "(Reading database ... 18408 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cron_3.0pl1-128ubuntu5_amd64.deb ...\n",
            "Unpacking cron (3.0pl1-128ubuntu5) ...\n",
            "Selecting previously unselected package libapparmor1:amd64.\n",
            "Preparing to unpack .../01-libapparmor1_2.11.0-2ubuntu17.1_amd64.deb ...\n",
            "Unpacking libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n",
            "Selecting previously unselected package libdbus-1-3:amd64.\n",
            "Preparing to unpack .../02-libdbus-1-3_1.10.22-1ubuntu1_amd64.deb ...\n",
            "Unpacking libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n",
            "Selecting previously unselected package dbus.\n",
            "Preparing to unpack .../03-dbus_1.10.22-1ubuntu1_amd64.deb ...\n",
            "Unpacking dbus (1.10.22-1ubuntu1) ...\n",
            "Selecting previously unselected package dirmngr.\n",
            "Preparing to unpack .../04-dirmngr_2.1.15-1ubuntu8.1_amd64.deb ...\n",
            "Unpacking dirmngr (2.1.15-1ubuntu8.1) ...\n",
            "Selecting previously unselected package distro-info-data.\n",
            "Preparing to unpack .../05-distro-info-data_0.36ubuntu0.2_all.deb ...\n",
            "Unpacking distro-info-data (0.36ubuntu0.2) ...\n",
            "Selecting previously unselected package libkmod2:amd64.\n",
            "Preparing to unpack .../06-libkmod2_24-1ubuntu2_amd64.deb ...\n",
            "Unpacking libkmod2:amd64 (24-1ubuntu2) ...\n",
            "Selecting previously unselected package kmod.\n",
            "Preparing to unpack .../07-kmod_24-1ubuntu2_amd64.deb ...\n",
            "Unpacking kmod (24-1ubuntu2) ...\n",
            "Selecting previously unselected package lsb-release.\n",
            "Preparing to unpack .../08-lsb-release_9.20160110ubuntu5_all.deb ...\n",
            "Unpacking lsb-release (9.20160110ubuntu5) ...\n",
            "Selecting previously unselected package libgirepository-1.0-1:amd64.\n",
            "Preparing to unpack .../09-libgirepository-1.0-1_1.54.1-1_amd64.deb ...\n",
            "Unpacking libgirepository-1.0-1:amd64 (1.54.1-1) ...\n",
            "Selecting previously unselected package gir1.2-glib-2.0:amd64.\n",
            "Preparing to unpack .../10-gir1.2-glib-2.0_1.54.1-1_amd64.deb ...\n",
            "Unpacking gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n",
            "Selecting previously unselected package iso-codes.\n",
            "Preparing to unpack .../11-iso-codes_3.75-1_all.deb ...\n",
            "Unpacking iso-codes (3.75-1) ...\n",
            "Selecting previously unselected package libdbus-glib-1-2:amd64.\n",
            "Preparing to unpack .../12-libdbus-glib-1-2_0.108-2_amd64.deb ...\n",
            "Unpacking libdbus-glib-1-2:amd64 (0.108-2) ...\n",
            "Selecting previously unselected package python-apt-common.\n",
            "Preparing to unpack .../13-python-apt-common_1.4.0~beta3build2_all.deb ...\n",
            "Unpacking python-apt-common (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python3-apt.\n",
            "Preparing to unpack .../14-python3-apt_1.4.0~beta3build2_amd64.deb ...\n",
            "Unpacking python3-apt (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python3-dbus.\n",
            "Preparing to unpack .../15-python3-dbus_1.2.4-1build3_amd64.deb ...\n",
            "Unpacking python3-dbus (1.2.4-1build3) ...\n",
            "Selecting previously unselected package python3-gi.\n",
            "Preparing to unpack .../16-python3-gi_3.24.1-2build1_amd64.deb ...\n",
            "Unpacking python3-gi (3.24.1-2build1) ...\n",
            "Selecting previously unselected package module-init-tools.\n",
            "Preparing to unpack .../17-module-init-tools_24-1ubuntu2_all.deb ...\n",
            "Unpacking module-init-tools (24-1ubuntu2) ...\n",
            "Selecting previously unselected package python-apt.\n",
            "Preparing to unpack .../18-python-apt_1.4.0~beta3build2_amd64.deb ...\n",
            "Unpacking python-apt (1.4.0~beta3build2) ...\n",
            "Selecting previously unselected package python-pycurl.\n",
            "Preparing to unpack .../19-python-pycurl_7.43.0-2build2_amd64.deb ...\n",
            "Unpacking python-pycurl (7.43.0-2build2) ...\n",
            "Selecting previously unselected package python-software-properties.\n",
            "Preparing to unpack .../20-python-software-properties_0.96.24.17_all.deb ...\n",
            "Unpacking python-software-properties (0.96.24.17) ...\n",
            "Selecting previously unselected package python3-software-properties.\n",
            "Preparing to unpack .../21-python3-software-properties_0.96.24.17_all.deb ...\n",
            "Unpacking python3-software-properties (0.96.24.17) ...\n",
            "Selecting previously unselected package software-properties-common.\n",
            "Preparing to unpack .../22-software-properties-common_0.96.24.17_all.deb ...\n",
            "Unpacking software-properties-common (0.96.24.17) ...\n",
            "Selecting previously unselected package unattended-upgrades.\n",
            "Preparing to unpack .../23-unattended-upgrades_0.98ubuntu1.1_all.deb ...\n",
            "Unpacking unattended-upgrades (0.98ubuntu1.1) ...\n",
            "Setting up python-apt-common (1.4.0~beta3build2) ...\n",
            "Setting up python3-apt (1.4.0~beta3build2) ...\n",
            "Setting up iso-codes (3.75-1) ...\n",
            "Setting up distro-info-data (0.36ubuntu0.2) ...\n",
            "Setting up python-pycurl (7.43.0-2build2) ...\n",
            "Setting up lsb-release (9.20160110ubuntu5) ...\n",
            "Setting up libgirepository-1.0-1:amd64 (1.54.1-1) ...\n",
            "Setting up libkmod2:amd64 (24-1ubuntu2) ...\n",
            "Setting up gir1.2-glib-2.0:amd64 (1.54.1-1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up libapparmor1:amd64 (2.11.0-2ubuntu17.1) ...\n",
            "Setting up unattended-upgrades (0.98ubuntu1.1) ...\n",
            "\n",
            "Creating config file /etc/apt/apt.conf.d/20auto-upgrades with new version\n",
            "\n",
            "Creating config file /etc/apt/apt.conf.d/50unattended-upgrades with new version\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up dirmngr (2.1.15-1ubuntu8.1) ...\n",
            "Setting up cron (3.0pl1-128ubuntu5) ...\n",
            "Adding group `crontab' (GID 102) ...\n",
            "Done.\n",
            "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
            "update-rc.d: warning: stop runlevel arguments (1) do not match cron Default-Stop values (none)\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libdbus-1-3:amd64 (1.10.22-1ubuntu1) ...\n",
            "Setting up kmod (24-1ubuntu2) ...\n",
            "Setting up libdbus-glib-1-2:amd64 (0.108-2) ...\n",
            "Setting up python3-gi (3.24.1-2build1) ...\n",
            "Setting up module-init-tools (24-1ubuntu2) ...\n",
            "Setting up python3-software-properties (0.96.24.17) ...\n",
            "Setting up dbus (1.10.22-1ubuntu1) ...\n",
            "Setting up python-apt (1.4.0~beta3build2) ...\n",
            "Setting up python3-dbus (1.2.4-1build3) ...\n",
            "Setting up python-software-properties (0.96.24.17) ...\n",
            "Setting up software-properties-common (0.96.24.17) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Processing triggers for dbus (1.10.22-1ubuntu1) ...\n",
            "gpg: keybox '/tmp/tmp7497nqz9/pubring.gpg' created\n",
            "gpg: /tmp/tmp7497nqz9/trustdb.gpg: trustdb created\n",
            "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "Selecting previously unselected package libfuse2:amd64.\n",
            "(Reading database ... 19816 files and directories currently installed.)\n",
            "Preparing to unpack .../libfuse2_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package fuse.\n",
            "Preparing to unpack .../fuse_2.9.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking fuse (2.9.7-1ubuntu1) ...\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.6.21-0ubuntu2_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n",
            "Setting up libfuse2:amd64 (2.9.7-1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up fuse (2.9.7-1ubuntu1) ...\n",
            "Setting up google-drive-ocamlfuse (0.6.21-0ubuntu2) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fg67a_4X2Dyb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_file_train = 'creditcard-train.csv'\n",
        "data_file_validation = 'creditcard-validation.csv'\n",
        "data_file_predict = 'creditcard-predict.csv'\n",
        "\n",
        "\n",
        "if use_cloud_storage:\n",
        "  !mkdir -p drive\n",
        "  !google-drive-ocamlfuse drive\n",
        "  sample_directory = './drive/ccsamples' #Directory to save sample output from generator in.\n",
        "  model_directory = './drive/ccmodels' #Directory to save trained model to.\n",
        "  data_directory = './drive/datasets' #Directory to save trained model to.\n",
        "\n",
        "else:\n",
        "  sample_directory = './ccsamples' #Directory to save sample output from generator in.\n",
        "  model_directory = './ccmodels' #Directory to save trained model to.\n",
        "  data_directory = './datasets' #Directory to save trained model to.\n",
        "\n",
        "data_full_path_train = data_directory + '/' + data_file_train\n",
        "data_full_path_validation = data_directory + '/' + data_file_validation\n",
        "data_full_path_predict = data_directory + '/' + data_file_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ogn9SBmurQuK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Generative Adversarial Network (DCGAN) Tutorial"
      ]
    },
    {
      "metadata": {
        "id": "NUnxlsO4rQuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorials walks through an implementation of DCGAN as described in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).\n",
        "\n",
        "To learn more about generative adversarial networks, see my [Medium post](https://medium.com/p/54deab2fce39) on them."
      ]
    },
    {
      "metadata": {
        "id": "V2Cy18bIrQuQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Import the libraries we will need.\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.contrib.slim as slim\n",
        "import os\n",
        "import scipy.misc\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zIy53iu4rQuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Prepare and load the dataset into what tensorflow will accept - \n",
        "\n",
        "Using guide from here:  https://learningtensorflow.com/ReadingFilesBasic/ \n",
        "\n",
        "and \n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/examples/tutorials/input_fn/boston.py\n",
        "\n",
        "Data looks like this:\n",
        "*   Time -  Number of seconds elapsed between this transaction and the first transaction in the dataset\n",
        "*   V1  -  V1 to V28 are all anonymised data features that are roughly converted into a real number between -1 and 1.  Not exactly normalised with mean of 0 but close.\n",
        "*   V2\n",
        "*   V3\n",
        "*   V4\n",
        "*   V5\n",
        "*   V6\n",
        "*   V7\n",
        "*   V8\n",
        "*   V9\n",
        "*   V10\n",
        "*   V11\n",
        "*   V12\n",
        "*   V13\n",
        "*   V14\n",
        "*   V15\n",
        "*   V16\n",
        "*   V17\n",
        "*   V18\n",
        "*   V19\n",
        "*   V20\n",
        "*   V21\n",
        "*   V22\n",
        "*   V23\n",
        "*   V24\n",
        "*   V25\n",
        "*   V26\n",
        "*   V27\n",
        "*   V28  \n",
        "*   Amount -  Transaction amount\n",
        "*   Class - 1 for fraudulent transactions, 0 otherwise"
      ]
    },
    {
      "metadata": {
        "id": "Y8o69NH0rQuX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import itertools\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "COLUMNS = [\"Time\", \"V1\", \"V2\", \"V3\",\n",
        "\"V4\", \"V5\", \"V6\", \"V7\",\n",
        "\"V8\", \"V9\", \"V10\", \"V11\",\n",
        "\"V12\", \"V13\", \"V14\", \"V15\",\n",
        "\"V16\", \"V17\", \"V18\", \"V19\",\n",
        "\"V20\", \"V21\", \"V22\", \"V23\",\n",
        "\"V24\", \"V25\", \"V26\", \"V27\",\n",
        "\"V28\", \"AmountTransaction\",\n",
        "\"Class\"]\n",
        "\n",
        "FEATURES = [\"Time\", \"V1\", \"V2\", \"V3\",\n",
        "\"V4\", \"V5\", \"V6\", \"V7\",\n",
        "\"V8\", \"V9\", \"V10\", \"V11\",\n",
        "\"V12\", \"V13\", \"V14\", \"V15\",\n",
        "\"V16\", \"V17\", \"V18\", \"V19\",\n",
        "\"V20\", \"V21\", \"V22\", \"V23\",\n",
        "\"V24\", \"V25\", \"V26\", \"V27\",\n",
        "\"V28\", \"AmountTransaction\"]\n",
        "\n",
        "LABEL = \"Class\"\n",
        "\n",
        "\n",
        "def get_input_fn(data_set, num_epochs=None, shuffle=True):\n",
        "  return tf.estimator.inputs.pandas_input_fn(\n",
        "      x=pd.DataFrame({k: data_set[k].values for k in FEATURES}),\n",
        "      y=pd.Series(data_set[LABEL].values),\n",
        "      num_epochs=num_epochs,\n",
        "      shuffle=shuffle)\n",
        "\n",
        "training_set = pd.read_csv(data_full_path_train, skipinitialspace=True,\n",
        "                             skiprows=1, names=COLUMNS)\n",
        "\n",
        "test_set = pd.read_csv(data_full_path_validation, skipinitialspace=True,\n",
        "                             skiprows=1, names=COLUMNS)\n",
        "\n",
        "prediction_set = pd.read_csv(data_full_path_predict, skipinitialspace=True,\n",
        "                             skiprows=1, names=COLUMNS)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yXK9oAq21uUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Check the data looks OK by printing the second row for the datafile"
      ]
    },
    {
      "metadata": {
        "id": "s9XtynJ_1xQx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "da19bbec-c20d-453e-f7b7-ad2b70b48c39"
      },
      "cell_type": "code",
      "source": [
        "print(training_set.iloc[[2]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Time        V1        V2        V3       V4        V5        V6        V7  \\\n",
            "2     1 -1.358354 -1.340163  1.773209  0.37978 -0.503198  1.800499  0.791461   \n",
            "\n",
            "         V8        V9  ...         V21       V22       V23       V24  \\\n",
            "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
            "\n",
            "        V25       V26       V27       V28  AmountTransaction  Class  \n",
            "2 -0.327642 -0.139097 -0.055353 -0.059752             378.66      0  \n",
            "\n",
            "[1 rows x 31 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yDnI3oLI59Xb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's train a basic classifier to see if we can realiably predict Class for fraud."
      ]
    },
    {
      "metadata": {
        "id": "pny5iNub6Gth",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2230
        },
        "outputId": "d2faef28-ab0a-44a1-8486-8451cd95c011"
      },
      "cell_type": "code",
      "source": [
        "  # Feature cols\n",
        "  feature_cols = [tf.feature_column.numeric_column(k) for k in FEATURES]\n",
        "\n",
        "  # Build 2 layer fully connected DNN with 10, 10 units respectively.\n",
        "  regressor = tf.estimator.DNNRegressor(feature_columns=feature_cols,\n",
        "                                        hidden_units=[10, 10],\n",
        "                                        model_dir=model_directory)\n",
        "\n",
        "  # Train\n",
        "  regressor.train(input_fn=get_input_fn(training_set), steps=5000)\n",
        "\n",
        "  # Evaluate loss over one epoch of test_set.\n",
        "  ev = regressor.evaluate(input_fn=get_input_fn(test_set, num_epochs=1, shuffle=False))\n",
        "  loss_score = ev[\"loss\"]\n",
        "  print(\"Loss: {0:f}\".format(loss_score))\n",
        "\n",
        "  # Print out predictions over a slice of prediction_set.\n",
        "  y = regressor.predict(input_fn=get_input_fn(prediction_set, num_epochs=1, shuffle=False))\n",
        "  \n",
        "  # .predict() returns an iterator of dicts; convert to a list and print\n",
        "  # predictions\n",
        "  predictions = list(p[\"predictions\"] for p in itertools.islice(y, 6))\n",
        "  print(\"Predictions: {}\".format(str(predictions)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fc9e66764d0>, '_evaluation_master': '', '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_device_fn': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': './drive/ccmodels', '_train_distribute': None, '_save_summary_steps': 100}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./drive/ccmodels/model.ckpt-5000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 5000 into ./drive/ccmodels/model.ckpt.\n",
            "INFO:tensorflow:loss = 22.028233, step = 5000\n",
            "INFO:tensorflow:global_step/sec: 116.182\n",
            "INFO:tensorflow:loss = 44.75009, step = 5100 (0.871 sec)\n",
            "INFO:tensorflow:global_step/sec: 119.691\n",
            "INFO:tensorflow:loss = 21.646843, step = 5200 (0.835 sec)\n",
            "INFO:tensorflow:global_step/sec: 84.4286\n",
            "INFO:tensorflow:loss = 37.175846, step = 5300 (1.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.5747\n",
            "INFO:tensorflow:loss = 56.98095, step = 5400 (1.022 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.484\n",
            "INFO:tensorflow:loss = 20.276848, step = 5500 (1.002 sec)\n",
            "INFO:tensorflow:global_step/sec: 57.8591\n",
            "INFO:tensorflow:loss = 28.810825, step = 5600 (1.740 sec)\n",
            "INFO:tensorflow:global_step/sec: 95.9605\n",
            "INFO:tensorflow:loss = 25.37023, step = 5700 (1.031 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.0922\n",
            "INFO:tensorflow:loss = 24.009167, step = 5800 (1.015 sec)\n",
            "INFO:tensorflow:global_step/sec: 100.186\n",
            "INFO:tensorflow:loss = 22.893831, step = 5900 (1.015 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.8054\n",
            "INFO:tensorflow:loss = 36.370483, step = 6000 (1.038 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.6654\n",
            "INFO:tensorflow:loss = 26.927193, step = 6100 (1.059 sec)\n",
            "INFO:tensorflow:global_step/sec: 101.723\n",
            "INFO:tensorflow:loss = 50.58922, step = 6200 (0.948 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.0765\n",
            "INFO:tensorflow:loss = 19.299892, step = 6300 (1.004 sec)\n",
            "INFO:tensorflow:global_step/sec: 100.555\n",
            "INFO:tensorflow:loss = 21.859692, step = 6400 (1.005 sec)\n",
            "INFO:tensorflow:global_step/sec: 101.276\n",
            "INFO:tensorflow:loss = 17.407837, step = 6500 (0.982 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.6171\n",
            "INFO:tensorflow:loss = 47.882313, step = 6600 (1.060 sec)\n",
            "INFO:tensorflow:global_step/sec: 101.719\n",
            "INFO:tensorflow:loss = 39.54874, step = 6700 (0.975 sec)\n",
            "INFO:tensorflow:global_step/sec: 98.861\n",
            "INFO:tensorflow:loss = 24.62836, step = 6800 (1.013 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.5172\n",
            "INFO:tensorflow:loss = 46.34998, step = 6900 (1.038 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.2294\n",
            "INFO:tensorflow:loss = 43.27311, step = 7000 (1.038 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.6617\n",
            "INFO:tensorflow:loss = 77.929405, step = 7100 (1.006 sec)\n",
            "INFO:tensorflow:global_step/sec: 95.3872\n",
            "INFO:tensorflow:loss = 17.490253, step = 7200 (1.067 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.4474\n",
            "INFO:tensorflow:loss = 28.673973, step = 7300 (1.039 sec)\n",
            "INFO:tensorflow:global_step/sec: 101.479\n",
            "INFO:tensorflow:loss = 53.67308, step = 7400 (1.029 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.3747\n",
            "INFO:tensorflow:loss = 42.12272, step = 7500 (0.987 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.7103\n",
            "INFO:tensorflow:loss = 41.814568, step = 7600 (1.046 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.3131\n",
            "INFO:tensorflow:loss = 46.159416, step = 7700 (1.044 sec)\n",
            "INFO:tensorflow:global_step/sec: 98.6995\n",
            "INFO:tensorflow:loss = 42.89733, step = 7800 (1.016 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.0173\n",
            "INFO:tensorflow:loss = 85.40483, step = 7900 (1.029 sec)\n",
            "INFO:tensorflow:global_step/sec: 98.0394\n",
            "INFO:tensorflow:loss = 42.66386, step = 8000 (1.042 sec)\n",
            "INFO:tensorflow:global_step/sec: 98.5684\n",
            "INFO:tensorflow:loss = 24.754324, step = 8100 (1.008 sec)\n",
            "INFO:tensorflow:global_step/sec: 101.385\n",
            "INFO:tensorflow:loss = 37.266403, step = 8200 (1.007 sec)\n",
            "INFO:tensorflow:global_step/sec: 100.997\n",
            "INFO:tensorflow:loss = 22.167988, step = 8300 (0.959 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.8435\n",
            "INFO:tensorflow:loss = 37.80709, step = 8400 (1.027 sec)\n",
            "INFO:tensorflow:global_step/sec: 98.9367\n",
            "INFO:tensorflow:loss = 30.571701, step = 8500 (1.011 sec)\n",
            "INFO:tensorflow:global_step/sec: 102.555\n",
            "INFO:tensorflow:loss = 24.733307, step = 8600 (0.976 sec)\n",
            "INFO:tensorflow:global_step/sec: 100.091\n",
            "INFO:tensorflow:loss = 28.110498, step = 8700 (0.999 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.9825\n",
            "INFO:tensorflow:loss = 29.906101, step = 8800 (1.019 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.5055\n",
            "INFO:tensorflow:loss = 28.885147, step = 8900 (1.005 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.4117\n",
            "INFO:tensorflow:loss = 21.82214, step = 9000 (1.000 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.6958\n",
            "INFO:tensorflow:loss = 16.790583, step = 9100 (1.003 sec)\n",
            "INFO:tensorflow:global_step/sec: 104.334\n",
            "INFO:tensorflow:loss = 26.180983, step = 9200 (0.968 sec)\n",
            "INFO:tensorflow:global_step/sec: 99.7659\n",
            "INFO:tensorflow:loss = 40.659306, step = 9300 (1.015 sec)\n",
            "INFO:tensorflow:global_step/sec: 93.1249\n",
            "INFO:tensorflow:loss = 21.136875, step = 9400 (1.081 sec)\n",
            "INFO:tensorflow:global_step/sec: 96.8714\n",
            "INFO:tensorflow:loss = 20.731905, step = 9500 (1.009 sec)\n",
            "INFO:tensorflow:global_step/sec: 98.1987\n",
            "INFO:tensorflow:loss = 25.812206, step = 9600 (1.039 sec)\n",
            "INFO:tensorflow:global_step/sec: 100.799\n",
            "INFO:tensorflow:loss = 22.812122, step = 9700 (0.974 sec)\n",
            "INFO:tensorflow:global_step/sec: 94.7666\n",
            "INFO:tensorflow:loss = 34.924545, step = 9800 (1.052 sec)\n",
            "INFO:tensorflow:global_step/sec: 97.8454\n",
            "INFO:tensorflow:loss = 22.639908, step = 9900 (1.019 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 10000 into ./drive/ccmodels/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 25.220581.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-08-20-12:04:24\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./drive/ccmodels/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2018-08-20-12:04:40\n",
            "INFO:tensorflow:Saving dict for global step 10000: average_loss = 0.3951239, global_step = 10000, label/mean = 0.0014703237, loss = 50.547684, prediction/mean = -0.34249592\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10000: ./drive/ccmodels/model.ckpt-10000\n",
            "Loss: 50.547684\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from ./drive/ccmodels/model.ckpt-10000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "Predictions: [array([-0.20936239], dtype=float32), array([-0.17127645], dtype=float32), array([-0.54920614], dtype=float32), array([-0.2586788], dtype=float32), array([-0.3026241], dtype=float32), array([-0.455212], dtype=float32)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rQyD49JmrQud",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "metadata": {
        "id": "mx77jz4crQue",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
        "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
        "     with tf.variable_scope(name):\n",
        "         f1 = 0.5 * (1 + leak)\n",
        "         f2 = 0.5 * (1 - leak)\n",
        "         return f1 * x + f2 * abs(x)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld-04pUwrQuk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the Adversarial Networks"
      ]
    },
    {
      "metadata": {
        "id": "q0RGiTTYrQul",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generator Network\n",
        "\n",
        "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
      ]
    },
    {
      "metadata": {
        "id": "bDBy-NcorQum",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(z):\n",
        "    \n",
        "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
        "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
        "    \n",
        "    gen1 = slim.convolution2d_transpose(\\\n",
        "        zCon,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
        "    \n",
        "    gen2 = slim.convolution2d_transpose(\\\n",
        "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
        "    \n",
        "    gen3 = slim.convolution2d_transpose(\\\n",
        "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
        "    \n",
        "    g_out = slim.convolution2d_transpose(\\\n",
        "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
        "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
        "        scope='g_out', weights_initializer=initializer)\n",
        "    \n",
        "    return g_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xLHyW51prQur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Discriminator Network\n",
        "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
      ]
    },
    {
      "metadata": {
        "id": "YWu5duzYrQut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator(bottom, reuse=False):\n",
        "    \n",
        "    dis1 = slim.convolution2d(bottom,16,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        biases_initializer=None,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
        "    \n",
        "    dis2 = slim.convolution2d(dis1,32,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
        "    \n",
        "    dis3 = slim.convolution2d(dis2,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
        "    \n",
        "    d_out = slim.fully_connected(slim.flatten(dis3),1,activation_fn=tf.nn.sigmoid,\\\n",
        "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
        "    \n",
        "    return d_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WvJGeO8trQuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Connecting them together"
      ]
    },
    {
      "metadata": {
        "id": "nJ5QOFzgrQuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "z_size = 100 #Size of z vector used for generator.\n",
        "\n",
        "#This initializaer is used to initialize all the weights of the network.\n",
        "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
        "\n",
        "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
        "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
        "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
        "\n",
        "Gz = generator(z_in) #Generates images from random z vectors\n",
        "Dx = discriminator(real_in) #Produces probabilities for real images\n",
        "Dg = discriminator(Gz,reuse=True) #Produces probabilities for generator images\n",
        "\n",
        "#These functions together define the optimization objective of the GAN.\n",
        "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
        "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
        "\n",
        "tvars = tf.trainable_variables()\n",
        "\n",
        "#The below code is responsible for applying gradient descent to update the GAN.\n",
        "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
        "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
        "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
        "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
        "\n",
        "update_D = trainerD.apply_gradients(d_grads)\n",
        "update_G = trainerG.apply_gradients(g_grads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9n6Z_3KrQu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the network\n",
        "Now that we have fully defined our network, it is time to train it!\n",
        "Check out the output in your Google Drive in the folder figs to see how it progresses."
      ]
    },
    {
      "metadata": {
        "id": "6sV2Ou1IrQu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4c690de0-cc7d-4a62-c921-0b69f2a7a741"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128 #Size of image batch to apply at each iteration.\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:  \n",
        "    sess.run(init)\n",
        "    #might be resuming a crashed training session so try loading the model\n",
        "    try:\n",
        "        print 'Loading Model...'\n",
        "        ckpt = tf.train.get_checkpoint_state(model_directory)\n",
        "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
        "    except:\n",
        "        print 'model didnt load, OK to ignore on first run'\n",
        "\n",
        "    for i in range(iterations):\n",
        "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
        "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
        "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
        "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
        "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
        "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
        "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
        "        if i % 100 == 0:\n",
        "            print \"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss)\n",
        "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
        "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
        "            if not os.path.exists(sample_directory):\n",
        "                os.makedirs(sample_directory)\n",
        "            #Save sample generator images for viewing training progress.\n",
        "            save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
        "        if i % 1000 == 0 and i != 0:\n",
        "            if not os.path.exists(model_directory):\n",
        "                os.makedirs(model_directory)\n",
        "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
        "            print \"Saved Model\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Model...\n",
            "INFO:tensorflow:Restoring parameters from ./drive/models/model-49000.cptk\n",
            "Gen Loss: 1.8926408 Disc Loss: 0.41932273\n",
            "Gen Loss: 2.5666914 Disc Loss: 0.4712001\n",
            "Gen Loss: 2.7967997 Disc Loss: 0.25285047\n",
            "Gen Loss: 1.9136124 Disc Loss: 0.7580761\n",
            "Gen Loss: 2.8225193 Disc Loss: 1.0394459\n",
            "Gen Loss: 1.8651094 Disc Loss: 0.293338\n",
            "Gen Loss: 2.0234199 Disc Loss: 0.40296856\n",
            "Gen Loss: 2.236509 Disc Loss: 0.44627184\n",
            "Gen Loss: 2.382635 Disc Loss: 0.42547378\n",
            "Gen Loss: 1.3726362 Disc Loss: 0.84920204\n",
            "Gen Loss: 1.0347916 Disc Loss: 0.651014\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qNH3k3ynrQvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using a trained network\n",
        "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
      ]
    },
    {
      "metadata": {
        "id": "hxm0jKpWrQvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "226a0e8f-e940-4c24-d658-01ac5af24a17"
      },
      "cell_type": "code",
      "source": [
        "batch_size_sample = 36\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:  \n",
        "    sess.run(init)\n",
        "    #Reload the model.\n",
        "    print 'Loading Model...'\n",
        "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
        "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
        "    \n",
        "    zs = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
        "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
        "    if not os.path.exists(sample_directory):\n",
        "        os.makedirs(sample_directory)\n",
        "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/sample'+str(i)+'.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Model...\n",
            "INFO:tensorflow:Restoring parameters from ./drive/models/model-1000.cptk\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}