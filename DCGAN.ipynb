{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/fmacrae/AI-Learning/blob/master/DCGAN.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "lzkpnqcI1YcA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###SETUP\n",
        "Choose whether to Connect to Google Drive to store the model.\n",
        "\n",
        "1.   If setting use_cloud_storage to false watch out for discconections to the VM - It's epherial so you'll lose all your checkpoints\n",
        "2.   Follow the instructions to link out to Google and supply the keys generated by pasting them into the prompts then press enter.  You'll be prompted to do this twice as two sets of permissions are needed.  Use the same google account for both.\n",
        "\n",
        "Also define how much training to do by selecting iterations value.  Stores the model every 1000 iterations so set the value to higher than 1001.\n",
        "\n",
        "Based on the work by Arthur Juliani, check out his other work! https://github.com/awjuliani/TF-Tutorials \n",
        "\n",
        "Changes:\n",
        "\n",
        "1.   Integrated with Google Drive to allow storage of model checkpoints and sample outputs\n",
        "2.   Moved onto Collabatory - Make sure you select runtime with GPU\n",
        "3.   Improved resiliancy by allowing it to restart from last checkpoint\n",
        "4.   Moved setup and training options to top for easier setup.\n",
        "\n",
        "Useful Links:  \n",
        "\n",
        "https://github.com/soumith/ganhacks\n",
        "\n",
        "https://sigmoidal.io/beginners-review-of-gan-architectures/\n",
        "\n",
        "https://www.toptal.com/machine-learning/generative-adversarial-networks\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "WcbWsqIlsqgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "use_cloud_storage = True ## change to false if you want to just store the images and checkpoints on the colab VM.  \n",
        "iterations = 1001 #Total number of iterations to use.  If you go lower than 1001 then no checkpoint is stored.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p1pn0bGw1WWT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "ff4de33a-4cf6-461b-c267-7eae42d5c6b8"
      },
      "cell_type": "code",
      "source": [
        "if use_cloud_storage:\n",
        "   !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "   !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "   !apt-get update -qq 2>&1 > /dev/null\n",
        "   !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "   from google.colab import auth\n",
        "   auth.authenticate_user()\n",
        "   from oauth2client.client import GoogleCredentials\n",
        "   creds = GoogleCredentials.get_application_default()\n",
        "   import getpass\n",
        "   !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "   vcode = getpass.getpass()\n",
        "   !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Fg67a_4X2Dyb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if use_cloud_storage:\n",
        "  !mkdir -p drive\n",
        "  !google-drive-ocamlfuse drive\n",
        "  sample_directory = './drive/figs' #Directory to save sample images from generator in.\n",
        "  model_directory = './drive/models' #Directory to save trained model to.\n",
        "else:\n",
        "  sample_directory = './figs' #Directory to save sample images from generator in.\n",
        "  model_directory = './models' #Directory to save trained model to."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ogn9SBmurQuK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Generative Adversarial Network (DCGAN) Tutorial"
      ]
    },
    {
      "metadata": {
        "id": "NUnxlsO4rQuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This tutorials walks through an implementation of DCGAN as described in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434).\n",
        "\n",
        "To learn more about generative adversarial networks, see Arthur's [Medium post](https://medium.com/p/54deab2fce39) on them."
      ]
    },
    {
      "metadata": {
        "id": "V2Cy18bIrQuQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Import the libraries we will need.\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.contrib.slim as slim\n",
        "import os\n",
        "import scipy.misc\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zIy53iu4rQuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will be using the MNIST dataset. input_data is a library that downloads the dataset and uzips it automatically."
      ]
    },
    {
      "metadata": {
        "id": "Y8o69NH0rQuX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "79c3bcb9-2a8c-4bbb-d4dc-79d1a57dd428"
      },
      "cell_type": "code",
      "source": [
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-6-f9584ef0ae80>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rQyD49JmrQud",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Helper Functions"
      ]
    },
    {
      "metadata": {
        "id": "mx77jz4crQue",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
        "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
        "     with tf.variable_scope(name):\n",
        "         f1 = 0.5 * (1 + leak)\n",
        "         f2 = 0.5 * (1 - leak)\n",
        "         return f1 * x + f2 * abs(x)\n",
        "    \n",
        "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
        "#They allow for saving sample images from the generator to follow progress\n",
        "def save_images(images, size, image_path):\n",
        "    return imsave(inverse_transform(images), size, image_path)\n",
        "\n",
        "def imsave(images, size, path):\n",
        "    return scipy.misc.imsave(path, merge(images, size))\n",
        "\n",
        "def inverse_transform(images):\n",
        "    return (images+1.)/2.\n",
        "\n",
        "def merge(images, size):\n",
        "    h, w = images.shape[1], images.shape[2]\n",
        "    img = np.zeros((h * size[0], w * size[1]))\n",
        "\n",
        "    for idx, image in enumerate(images):\n",
        "        i = idx % size[1]\n",
        "        j = idx // size[1]\n",
        "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
        "\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ld-04pUwrQuk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the Adversarial Networks"
      ]
    },
    {
      "metadata": {
        "id": "q0RGiTTYrQul",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generator Network\n",
        "\n",
        "The generator takes a vector of random numbers and transforms it into a 32x32 image. Each layer in the network involves a strided  transpose convolution, batch normalization, and rectified nonlinearity. Tensorflow's slim library allows us to easily define each of these layers."
      ]
    },
    {
      "metadata": {
        "id": "bDBy-NcorQum",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generator(z):\n",
        "    \n",
        "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
        "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
        "    \n",
        "    gen1 = slim.convolution2d_transpose(\\\n",
        "        zCon,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
        "    \n",
        "    gen2 = slim.convolution2d_transpose(\\\n",
        "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
        "    \n",
        "    gen3 = slim.convolution2d_transpose(\\\n",
        "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
        "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
        "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
        "    \n",
        "    g_out = slim.convolution2d_transpose(\\\n",
        "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
        "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
        "        scope='g_out', weights_initializer=initializer)\n",
        "    \n",
        "    return g_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xLHyW51prQur",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Discriminator Network\n",
        "The discriminator network takes as input a 32x32 image and transforms it into a single valued probability of being generated from real-world data. Again we use tf.slim to define the convolutional layers, batch normalization, and weight initialization."
      ]
    },
    {
      "metadata": {
        "id": "YWu5duzYrQut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discriminator(bottom, reuse=False):\n",
        "    \n",
        "    dis1 = slim.convolution2d(bottom,16,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        biases_initializer=None,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv1',weights_initializer=initializer)\n",
        "    \n",
        "    dis2 = slim.convolution2d(dis1,32,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv2', weights_initializer=initializer)\n",
        "    \n",
        "    dis3 = slim.convolution2d(dis2,64,[4,4],stride=[2,2],padding=\"SAME\",\\\n",
        "        normalizer_fn=slim.batch_norm,activation_fn=lrelu,\\\n",
        "        reuse=reuse,scope='d_conv3',weights_initializer=initializer)\n",
        "    \n",
        "    d_out = slim.fully_connected(slim.flatten(dis3),1,activation_fn=tf.nn.sigmoid,\\\n",
        "        reuse=reuse,scope='d_out', weights_initializer=initializer)\n",
        "    \n",
        "    return d_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WvJGeO8trQuy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Connecting them together"
      ]
    },
    {
      "metadata": {
        "id": "nJ5QOFzgrQuz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "z_size = 100 #Size of z vector used for generator.\n",
        "\n",
        "#This initializaer is used to initialize all the weights of the network.\n",
        "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
        "\n",
        "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
        "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
        "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
        "\n",
        "Gz = generator(z_in) #Generates images from random z vectors\n",
        "Dx = discriminator(real_in) #Produces probabilities for real images\n",
        "Dg = discriminator(Gz,reuse=True) #Produces probabilities for generator images\n",
        "\n",
        "#These functions together define the optimization objective of the GAN.\n",
        "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
        "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
        "\n",
        "tvars = tf.trainable_variables()\n",
        "\n",
        "#The below code is responsible for applying gradient descent to update the GAN.\n",
        "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
        "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
        "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
        "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
        "\n",
        "update_D = trainerD.apply_gradients(d_grads)\n",
        "update_G = trainerG.apply_gradients(g_grads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g9n6Z_3KrQu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training the network\n",
        "Now that we have fully defined our network, it is time to train it!\n",
        "Check out the output in your Google Drive in the folder 'figs' to see how it progresses."
      ]
    },
    {
      "metadata": {
        "id": "6sV2Ou1IrQu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "4c690de0-cc7d-4a62-c921-0b69f2a7a741"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128 #Size of image batch to apply at each iteration.\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:  \n",
        "    sess.run(init)\n",
        "    #might be resuming a crashed training session so try loading the model\n",
        "    try:\n",
        "        print 'Loading Model...'\n",
        "        ckpt = tf.train.get_checkpoint_state(model_directory)\n",
        "        saver.restore(sess,ckpt.model_checkpoint_path)\n",
        "    except:\n",
        "        print 'model didnt load, OK to ignore on first run'\n",
        "\n",
        "    for i in range(iterations):\n",
        "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
        "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
        "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
        "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
        "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
        "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
        "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
        "        if i % 100 == 0:\n",
        "            print \"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss)\n",
        "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
        "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
        "            if not os.path.exists(sample_directory):\n",
        "                os.makedirs(sample_directory)\n",
        "            #Save sample generator images for viewing training progress.\n",
        "            save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
        "        if i % 1000 == 0 and i != 0:\n",
        "            if not os.path.exists(model_directory):\n",
        "                os.makedirs(model_directory)\n",
        "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
        "            print \"Saved Model\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Model...\n",
            "INFO:tensorflow:Restoring parameters from ./drive/models/model-49000.cptk\n",
            "Gen Loss: 1.8926408 Disc Loss: 0.41932273\n",
            "Gen Loss: 2.5666914 Disc Loss: 0.4712001\n",
            "Gen Loss: 2.7967997 Disc Loss: 0.25285047\n",
            "Gen Loss: 1.9136124 Disc Loss: 0.7580761\n",
            "Gen Loss: 2.8225193 Disc Loss: 1.0394459\n",
            "Gen Loss: 1.8651094 Disc Loss: 0.293338\n",
            "Gen Loss: 2.0234199 Disc Loss: 0.40296856\n",
            "Gen Loss: 2.236509 Disc Loss: 0.44627184\n",
            "Gen Loss: 2.382635 Disc Loss: 0.42547378\n",
            "Gen Loss: 1.3726362 Disc Loss: 0.84920204\n",
            "Gen Loss: 1.0347916 Disc Loss: 0.651014\n",
            "Saved Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qNH3k3ynrQvD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using a trained network\n",
        "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
      ]
    },
    {
      "metadata": {
        "id": "hxm0jKpWrQvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "226a0e8f-e940-4c24-d658-01ac5af24a17"
      },
      "cell_type": "code",
      "source": [
        "batch_size_sample = 36\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:  \n",
        "    sess.run(init)\n",
        "    #Reload the model.\n",
        "    print 'Loading Model...'\n",
        "    ckpt = tf.train.get_checkpoint_state(model_directory)\n",
        "    saver.restore(sess,ckpt.model_checkpoint_path)\n",
        "    \n",
        "    zs = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
        "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
        "    if not os.path.exists(sample_directory):\n",
        "        os.makedirs(sample_directory)\n",
        "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/sample'+str(i)+'.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Model...\n",
            "INFO:tensorflow:Restoring parameters from ./drive/models/model-1000.cptk\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}